# Computer-Vision
## Introduction
This project focused on implementing deep learning models for the classification of wheat plant diseases using the publicly available dataset from Kaggle titled Wheat Plant Diseases. The work was inspired by the Nature paper “A deep learning based approach for automated plant disease classification using vision transformer” and aimed to implement its four architectural blocks
## Implementation
Block I was a conventional CNN model, consisting of two sequential convolutional layers followed by LeakyReLU activation and max pooling, repeated twice, mimicking the VGG-style structure for local feature extraction. Block II employed a pure Transformer-based approach, where input images were divided into patches using a convolutional projection, enriched with positional embeddings, and processed through two stacked Transformer encoder blocks for global attention modeling. Block III represented a hybrid CNN→Transformer model, where convolutional layers first extracted low-level spatial features which were then projected and passed through a pretrained Vision Transformer encoder to capture global relationships. In contrast, Block IV followed a Transformer→CNN design, where the image was first processed by a Transformer block to model contextual dependencies, and the output was reshaped and refined using a CNN block before classification. Each block was trained and evaluated on a 15-class wheat plant disease dataset, allowing comparative analysis of local vs. global feature modeling strategies.
## Results Summarization
•	The CNN + ViT hybrid model achieved the highest test accuracy of 90% but required the longest training time of 5 hours on a a RTX-4070 S GPU. This result is expected because the architecture combines strong local feature extraction (via CNN layers) with a powerful pretrained Vision Transformer (ViT) encoder that models long-range dependencies. The high accuracy is likely attributed to the use of pretrained weights from large-scale datasets like ImageNet, which help generalize better, especially in fine-grained classification tasks like plant disease detection.
•	The ViT + CNN model reached an accuracy of 80%, training on a T4 GPU. In this architecture, the transformer block processes the image first, and then a CNN block is used to refine local spatial features. This approach benefits from global attention early in the network and spatial resolution later, resulting in a balanced architecture. Although it trained on a lower-powered GPU, its performance and training duration were relatively efficient, likely between 3–4 hours.
•	The ViT + ViT model achieved a test accuracy of around 69% with a training time of 2.5 hours on a T4 GPU. While transformers are excellent at modeling relationships between distant parts of the image, this architecture lacks the spatial inductive biases that CNNs 
